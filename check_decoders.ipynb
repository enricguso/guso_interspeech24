{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba594f87-4601-46cb-8a1e-92df7915077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "import scipy.signal as sig\n",
    "import matplotlib.pyplot as plt\n",
    "import mat73\n",
    "import os\n",
    "import masp as srs\n",
    "import copy\n",
    "from os.path import join as pjoin\n",
    "import librosa as lsa\n",
    "import soundfile as sf\n",
    "import sofar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab5fa57-c753-4585-b163-032461bb903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_echogram(anechoic_echogram):\n",
    "    nSrc = anechoic_echogram.shape[0]\n",
    "    nRec = anechoic_echogram.shape[1]\n",
    "    nBands = anechoic_echogram.shape[2]\n",
    "    # Returns the \"anechoic\" version of an echogram\n",
    "    # Should keep the receiver directivy\n",
    "    for src in range(nSrc):\n",
    "        for rec in range(nRec):\n",
    "            for band in range(nBands):\n",
    "                anechoic_echogram[src, rec, band].time = anechoic_echogram[src, rec, band].time[:2]\n",
    "                anechoic_echogram[src, rec, band].coords = anechoic_echogram[src, rec, band].coords[:2, :]\n",
    "                anechoic_echogram[src, rec, band].value = anechoic_echogram[src, rec, band].value[:2,:]\n",
    "                anechoic_echogram[src, rec, band].order = anechoic_echogram[src, rec, band].order[:2,:]\n",
    "    return anechoic_echogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed68807f-9b67-49d3-b0d4-9949b0ee85cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_2_ku_ears(head_pos,head_orient):\n",
    "# based on head pos and orientation, compute coordinates of ears\n",
    "    ear_distance_ku100=0.0875\n",
    "    theta = (head_orient[0]) * np.pi / 180\n",
    "    R_ear = [head_pos[0] - ear_distance_ku100 * np.sin(theta),\n",
    "              head_pos[1] + ear_distance_ku100 * np.cos(theta), \n",
    "              head_pos[2]]\n",
    "    L_ear = [head_pos[0] + ear_distance_ku100 * np.sin(theta),\n",
    "              head_pos[1] - ear_distance_ku100 * np.cos(theta), \n",
    "              head_pos[2]]\n",
    "    return [L_ear,R_ear]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8343071-f7f4-49d7-93d3-ed8c30931ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scene(room_dims,head_pos,head_orient,l_mic_pos,l_src_pos,perspective=\"xy\"):\n",
    "#   function to plot the designed scene\n",
    "#   room_dims - dimensions of the room [x,y,z]\n",
    "#   head_pos - head position [x,y,z]\n",
    "#   head_orient - [az,el]\n",
    "#   l_src_pos - list of source positions [[x,y,z],...,[x,y,z]]\n",
    "#   perspective - which two dimensions to show \n",
    "    if perspective==\"xy\":\n",
    "        dim1=1\n",
    "        dim2=0\n",
    "    elif perspective==\"yz\":\n",
    "        dim1=2\n",
    "        dim2=1\n",
    "    elif perspective==\"xz\":\n",
    "        dim1=2\n",
    "        dim2=0\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    plt.xlim((0,room_dims[dim1]))\n",
    "    plt.ylim((0,room_dims[dim2]))\n",
    "    plt.axvline(head_pos[dim1], color='y') # horizontal lines\n",
    "    plt.axhline(head_pos[dim2], color='y') # vertical lines\n",
    "    plt.grid(True)\n",
    "    # plot sources and receivers\n",
    "    plt.plot(head_pos[dim1],head_pos[dim2], \"o\", ms=10, mew=2, color=\"black\")\n",
    "    # plot ears\n",
    "    plt.plot(l_mic_pos[0][dim1],l_mic_pos[0][dim2], \"o\", ms=3, mew=2, color=\"blue\")# left ear in blue\n",
    "    plt.plot(l_mic_pos[1][dim1],l_mic_pos[1][dim2], \"o\", ms=3, mew=2, color=\"red\")# right ear in red\n",
    "\n",
    "    for i,src_pos in enumerate(l_src_pos):\n",
    "        plt.plot(src_pos[dim1],src_pos[dim2], \"o\", ms=10, mew=2, color=\"red\")\n",
    "        plt.annotate(str(i), (src_pos[dim1],src_pos[dim2]))\n",
    "    # plot head orientation if looking from above \n",
    "    if perspective==\"xy\":\n",
    "        plt.plot(head_pos[dim1],head_pos[dim2], marker=(1, 1, -head_orient[0]), ms=20, mew=2,color=\"black\")\n",
    "\n",
    "    ax.set_aspect('equal', adjustable='box')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd0be27-6356-4b5c-a15e-638192ac091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(src, headC, headOrient, room, rt60, band_centerfreqs, maxlim, ambi_order, fs_rir, decoder, speech):\n",
    "    mic = np.array(head_2_ku_ears(headC,headOrient)) # we get BiMagLS mic points \n",
    "    mic = np.vstack((mic, headC)) # we add the head center microphone for MagLS decoders\n",
    "    nRec = mic.shape[0]\n",
    "    nSrc = src.shape[0]\n",
    "    abs_walls,rt60_true = srs.find_abs_coeffs_from_rt(room, rt60)\n",
    "    # Small correction for sound absorption coefficients:\n",
    "    if sum(rt60_true-rt60>0.05*rt60_true)>0 :\n",
    "        abs_walls,rt60_true = srs.find_abs_coeffs_from_rt(room, rt60_true + abs(rt60-rt60_true))\n",
    "    # Generally, we simulate up to RT60:\n",
    "    limits = np.minimum(rt60, maxlim)\n",
    "    # Compute IRs with MASP at 48k:\n",
    "    abs_echograms = srs.compute_echograms_sh(room, src, mic, abs_walls, limits, ambi_order, headOrient)\n",
    "    ane_echograms = crop_echogram(copy.deepcopy(abs_echograms))\n",
    "    mic_rirs = srs.render_rirs_sh(abs_echograms, band_centerfreqs, fs_rir)/np.sqrt(4*np.pi)\n",
    "    ane_rirs = srs.render_rirs_sh(ane_echograms, band_centerfreqs, fs_rir)/np.sqrt(4*np.pi)\n",
    "    # Pad anechoic rirs so we don't loose alignment when convolving\n",
    "    zeros_to_pad = len(mic_rirs) - len(ane_rirs)\n",
    "    zeros_to_pad = np.zeros((zeros_to_pad, mic_rirs.shape[1], mic_rirs.shape[2], mic_rirs.shape[3]))\n",
    "    ane_rirs = np.concatenate((ane_rirs, zeros_to_pad))\n",
    "    bin_ir = np.array([sig.fftconvolve(np.squeeze(mic_rirs[:,:,0, 0]), decoder[:,:,0], 'full', 0).sum(1),\n",
    "                    sig.fftconvolve(np.squeeze(mic_rirs[:,:,1, 0]), decoder[:,:,1], 'full', 0).sum(1)])\n",
    "    bin_aneIR = np.array([sig.fftconvolve(np.squeeze(ane_rirs[:,:,0, 0]), decoder[:,:,0], 'full', 0).sum(1),\n",
    "                    sig.fftconvolve(np.squeeze(ane_rirs[:,:,1, 0]), decoder[:,:,1], 'full', 0).sum(1)])\n",
    "    reverberant_src = np.array([sig.fftconvolve(speech, bin_ir[0, :], 'same'), sig.fftconvolve(speech, bin_ir[1, :], 'same')])\n",
    "    anechoic_src = np.array([sig.fftconvolve(speech, bin_aneIR[0, :], 'same'), sig.fftconvolve(speech, bin_aneIR[1, :], 'same')])\n",
    "    return reverberant_src, anechoic_src, mic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35de1e0e-6893-425a-bd8c-364e240de0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_path = pjoin('decoders_ord10', 'Ku100_ALFE_Window_sinEQ_bimag.mat') #10th order BimagLS decoder del KU100 sin HA a 48kHz\n",
    "decoder = mat73.loadmat(decoder_path)['hnm']\n",
    "decoder = np.roll(decoder,500,axis=0)\n",
    "maxlim = 2 # maximum reflection time in seconds. Stop simulating if it goes beyond that time.\n",
    "ambi_order = 10 # ambisonics order\n",
    "\n",
    "headC_x = 2.0  \n",
    "headC_y = 2.0\n",
    "headC_z = 1.0\n",
    "headOrient_azi = 0.0\n",
    "headOrient_ele = 0.0\n",
    "headC = np.array([headC_x, headC_y, headC_z])\n",
    "headOrient = np.array([headOrient_azi,headOrient_ele])\n",
    "src = np.array([[3,\t3, 1]]) #speech speaker position following convention:\n",
    "\n",
    "room = np.array([6, 4, 2.5]) #dimensions\n",
    "rt60=np.array([0.01])\n",
    "band_centerfreqs=np.array([1000])\n",
    "fs_rir = 48000\n",
    "fs_target = fs_rir\n",
    "\n",
    "speech, fs_speech = lsa.load('ane_speech.wav', sr=fs_rir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4de5871-fc4c-43aa-9775-43178e59eb00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headOrient = np.array([0.,0.])\n",
    "_, deg0, mic0 = process(src, headC, headOrient, room, rt60, band_centerfreqs, maxlim, ambi_order, fs_rir, decoder, speech)\n",
    "plot_scene(room,headC, headOrient,mic0,src,perspective=\"xy\")\n",
    "Audio(deg0, rate=fs_rir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ae6f31-5f73-4836-a053-a06725836702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load sofa file\n",
    "import sofar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f28528-27e3-4119-ae61-ae6e21a8e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sofar.read_sofa('sofa_files/KU100_New_128_noALFE_cut_now.sofa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6b113-1cb9-4c11-8640-73dfaffba28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s.Data_IR[-5][0])\n",
    "plt.plot(s.Data_IR[-5][1])\n",
    "print(s.SourcePosition[-5])\n",
    "plt.legend(['left', 'right']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72b3015-9f3f-412d-85bc-d0cf2058a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "osig = np.array([sig.fftconvolve(speech, s.Data_IR[-5][0], 'same'), sig.fftconvolve(speech, s.Data_IR[-5][1], 'same')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247f643f-c6f3-4ac7-a34d-561627e0619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(deg0, rate=fs_rir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec48eb5d-f7e4-48aa-861f-e4bf95f82c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(osig, rate=fs_rir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3423c0a6-295b-4635-837a-c6dcdca679de",
   "metadata": {},
   "outputs": [],
   "source": [
    "headOrient = np.array([90.,0.])\n",
    "_, deg90, mic90 = process(src, headC, headOrient, room, rt60, band_centerfreqs, maxlim, ambi_order, fs_rir, decoder, speech)\n",
    "plot_scene(room,headC, headOrient,mic90,src,perspective=\"xy\")\n",
    "#Audio(deg90, rate=fs_rir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c452c58-2466-42e1-9805-47715639aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s.SourcePosition[9])\n",
    "osig90 = np.array([sig.fftconvolve(speech, s.Data_IR[9][0], 'same'), sig.fftconvolve(speech, s.Data_IR[9][1], 'same')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493e8abf-c3d3-4324-9023-f3ab69dc8514",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(deg90, rate=fs_rir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae14a390-91b9-4a7f-a319-3c0e9781e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(osig90, rate=fs_rir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "in24",
   "language": "python",
   "name": "in24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
